{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv3D, MaxPooling3D, BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import img_to_array\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\n(320, 320, 1)\nFinished feature extraction from  29  files\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "\n",
    "# Set the path to the full audio dataset \n",
    "fulldatasetpath = r'./images/'\n",
    "\n",
    "metadata = pd.read_csv(r'./metadatas/matadata.csv')\n",
    "\n",
    "features = []\n",
    "\n",
    "# Iterate through each sound file and extract the features \n",
    "for index, row in metadata.iterrows():\n",
    "    file_name = os.path.join(os.path.abspath(fulldatasetpath), str(row[\"category\"]), 'Case ' + str(row[\"number\"]) + ' ' + str(row[\"ID\"]))\n",
    "    mri = []\n",
    "    for i in range(1, 29):\n",
    "        im = image.load_img(file_name + r'/' + str(i) + r'.jpg', target_size = (320, 320), color_mode = 'grayscale')\n",
    "        im = img_to_array(im)\n",
    "        print(im.shape)\n",
    "        mri.append(im)\n",
    "    \n",
    "    class_label = row[\"category\"]\n",
    "    features.append([mri, class_label])\n",
    "# Convert into a Panda dataframe \n",
    "featuresdf = pd.DataFrame(features, columns=['feature','class_label'])\n",
    "\n",
    "print('Finished feature extraction from ', len(featuresdf), ' files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(28, 320, 320, 1)\n"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert features and corresponding classification labels into numpy arrays\n",
    "X = np.array(featuresdf.feature.tolist())\n",
    "y = np.array(featuresdf.class_label.tolist())\n",
    "print(X[0].shape)\n",
    "# Encode the classification labels\n",
    "le = LabelEncoder()\n",
    "yy = to_categorical(le.fit_transform(y)) \n",
    "\n",
    "# split the dataset \n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(28, 320, 320, 1)\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics \n",
    "\n",
    "a, num_channels, num_rows, num_columns, b = X.shape\n",
    "print(X[0].shape)\n",
    "x_train = x_train.reshape(x_train.shape[0],num_channels, num_rows, num_columns, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0],num_channels, num_rows, num_columns, 1)\n",
    "\n",
    "num_labels = yy.shape[1]\n",
    "filter_size = 2\n",
    "\n",
    "# Construct model \n",
    "model = Sequential()\n",
    "model.add(Conv3D(filters=8, kernel_size=3, input_shape=(num_channels, num_rows, num_columns, 1), activation='relu'))\n",
    "model.add(MaxPooling3D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv3D(filters=16, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling3D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv3D(filters=32, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling3D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(num_labels, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv3d_4 (Conv3D)            (None, 26, 318, 318, 8)   224       \n_________________________________________________________________\nmax_pooling3d_4 (MaxPooling3 (None, 13, 159, 159, 8)   0         \n_________________________________________________________________\ndropout_7 (Dropout)          (None, 13, 159, 159, 8)   0         \n_________________________________________________________________\nconv3d_5 (Conv3D)            (None, 11, 157, 157, 16)  3472      \n_________________________________________________________________\nmax_pooling3d_5 (MaxPooling3 (None, 5, 78, 78, 16)     0         \n_________________________________________________________________\ndropout_8 (Dropout)          (None, 5, 78, 78, 16)     0         \n_________________________________________________________________\nconv3d_6 (Conv3D)            (None, 3, 76, 76, 32)     13856     \n_________________________________________________________________\nmax_pooling3d_6 (MaxPooling3 (None, 1, 38, 38, 32)     0         \n_________________________________________________________________\ndropout_9 (Dropout)          (None, 1, 38, 38, 32)     0         \n_________________________________________________________________\nbatch_normalization_2 (Batch (None, 1, 38, 38, 32)     128       \n_________________________________________________________________\nflatten_2 (Flatten)          (None, 46208)             0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 512)               23659008  \n_________________________________________________________________\nactivation_4 (Activation)    (None, 512)               0         \n_________________________________________________________________\ndropout_10 (Dropout)         (None, 512)               0         \n_________________________________________________________________\ndense_6 (Dense)              (None, 512)               262656    \n_________________________________________________________________\nactivation_5 (Activation)    (None, 512)               0         \n_________________________________________________________________\ndropout_11 (Dropout)         (None, 512)               0         \n_________________________________________________________________\ndense_7 (Dense)              (None, 512)               262656    \n_________________________________________________________________\nactivation_6 (Activation)    (None, 512)               0         \n_________________________________________________________________\ndropout_12 (Dropout)         (None, 512)               0         \n_________________________________________________________________\ndense_8 (Dense)              (None, 2)                 1026      \n=================================================================\nTotal params: 24,203,026\nTrainable params: 24,202,962\nNon-trainable params: 64\n_________________________________________________________________\n6/6 [==============================] - 0s 62ms/step\nPre-training accuracy: 83.3333%\n"
    }
   ],
   "source": [
    "\n",
    "# Display model architecture summary \n",
    "model.summary()\n",
    "\n",
    "# Calculate pre-training accuracy \n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "accuracy = 100 * score[1]\n",
    "\n",
    "print(\"Pre-training accuracy: %.4f%%\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "333\n\nEpoch 00901: val_loss did not improve from 0.01806\nEpoch 902/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8117 - val_accuracy: 0.8333\n\nEpoch 00902: val_loss did not improve from 0.01806\nEpoch 903/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8321 - val_accuracy: 0.8333\n\nEpoch 00903: val_loss did not improve from 0.01806\nEpoch 904/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8303 - val_accuracy: 0.8333\n\nEpoch 00904: val_loss did not improve from 0.01806\nEpoch 905/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8018 - val_accuracy: 0.8333\n\nEpoch 00905: val_loss did not improve from 0.01806\nEpoch 906/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7895 - val_accuracy: 0.8333\n\nEpoch 00906: val_loss did not improve from 0.01806\nEpoch 907/1000\n23/23 [==============================] - 1s 27ms/step - loss: 9.3294e-08 - accuracy: 1.0000 - val_loss: 8.8120 - val_accuracy: 0.8333\n\nEpoch 00907: val_loss did not improve from 0.01806\nEpoch 908/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8203 - val_accuracy: 0.8333\n\nEpoch 00908: val_loss did not improve from 0.01806\nEpoch 909/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8088 - val_accuracy: 0.8333\n\nEpoch 00909: val_loss did not improve from 0.01806\nEpoch 910/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8127 - val_accuracy: 0.8333\n\nEpoch 00910: val_loss did not improve from 0.01806\nEpoch 911/1000\n23/23 [==============================] - 1s 26ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7974 - val_accuracy: 0.8333\n\nEpoch 00911: val_loss did not improve from 0.01806\nEpoch 912/1000\n23/23 [==============================] - 1s 26ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7901 - val_accuracy: 0.8333\n\nEpoch 00912: val_loss did not improve from 0.01806\nEpoch 913/1000\n23/23 [==============================] - 1s 26ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8007 - val_accuracy: 0.8333\n\nEpoch 00913: val_loss did not improve from 0.01806\nEpoch 914/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7940 - val_accuracy: 0.8333\n\nEpoch 00914: val_loss did not improve from 0.01806\nEpoch 915/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8033 - val_accuracy: 0.8333\n\nEpoch 00915: val_loss did not improve from 0.01806\nEpoch 916/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7882 - val_accuracy: 0.8333\n\nEpoch 00916: val_loss did not improve from 0.01806\nEpoch 917/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.7933 - val_accuracy: 0.8333\n\nEpoch 00917: val_loss did not improve from 0.01806\nEpoch 918/1000\n23/23 [==============================] - 1s 26ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8108 - val_accuracy: 0.8333\n\nEpoch 00918: val_loss did not improve from 0.01806\nEpoch 919/1000\n23/23 [==============================] - 1s 26ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8236 - val_accuracy: 0.8333\n\nEpoch 00919: val_loss did not improve from 0.01806\nEpoch 920/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8301 - val_accuracy: 0.8333\n\nEpoch 00920: val_loss did not improve from 0.01806\nEpoch 921/1000\n23/23 [==============================] - 1s 26ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8364 - val_accuracy: 0.8333\n\nEpoch 00921: val_loss did not improve from 0.01806\nEpoch 922/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8667 - val_accuracy: 0.8333\n\nEpoch 00922: val_loss did not improve from 0.01806\nEpoch 923/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8669 - val_accuracy: 0.8333\n\nEpoch 00923: val_loss did not improve from 0.01806\nEpoch 924/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8820 - val_accuracy: 0.8333\n\nEpoch 00924: val_loss did not improve from 0.01806\nEpoch 925/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9130 - val_accuracy: 0.8333\n\nEpoch 00925: val_loss did not improve from 0.01806\nEpoch 926/1000\n23/23 [==============================] - 1s 26ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9142 - val_accuracy: 0.8333\n\nEpoch 00926: val_loss did not improve from 0.01806\nEpoch 927/1000\n23/23 [==============================] - 1s 26ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9223 - val_accuracy: 0.8333\n\nEpoch 00927: val_loss did not improve from 0.01806\nEpoch 928/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9427 - val_accuracy: 0.8333\n\nEpoch 00928: val_loss did not improve from 0.01806\nEpoch 929/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9640 - val_accuracy: 0.8333\n\nEpoch 00929: val_loss did not improve from 0.01806\nEpoch 930/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.0011 - val_accuracy: 0.8333\n\nEpoch 00930: val_loss did not improve from 0.01806\nEpoch 931/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9771 - val_accuracy: 0.8333\n\nEpoch 00931: val_loss did not improve from 0.01806\nEpoch 932/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9788 - val_accuracy: 0.8333\n\nEpoch 00932: val_loss did not improve from 0.01806\nEpoch 933/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9798 - val_accuracy: 0.8333\n\nEpoch 00933: val_loss did not improve from 0.01806\nEpoch 934/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9843 - val_accuracy: 0.8333\n\nEpoch 00934: val_loss did not improve from 0.01806\nEpoch 935/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9636 - val_accuracy: 0.8333\n\nEpoch 00935: val_loss did not improve from 0.01806\nEpoch 936/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9906 - val_accuracy: 0.8333\n\nEpoch 00936: val_loss did not improve from 0.01806\nEpoch 937/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9886 - val_accuracy: 0.8333\n\nEpoch 00937: val_loss did not improve from 0.01806\nEpoch 938/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.0370 - val_accuracy: 0.8333\n\nEpoch 00938: val_loss did not improve from 0.01806\nEpoch 939/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.0476 - val_accuracy: 0.8333\n\nEpoch 00939: val_loss did not improve from 0.01806\nEpoch 940/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.0344 - val_accuracy: 0.8333\n\nEpoch 00940: val_loss did not improve from 0.01806\nEpoch 941/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.0191 - val_accuracy: 0.8333\n\nEpoch 00941: val_loss did not improve from 0.01806\nEpoch 942/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.0345 - val_accuracy: 0.8333\n\nEpoch 00942: val_loss did not improve from 0.01806\nEpoch 943/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.0382 - val_accuracy: 0.8333\n\nEpoch 00943: val_loss did not improve from 0.01806\nEpoch 944/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9585 - val_accuracy: 0.8333\n\nEpoch 00944: val_loss did not improve from 0.01806\nEpoch 945/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9843 - val_accuracy: 0.8333\n\nEpoch 00945: val_loss did not improve from 0.01806\nEpoch 946/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9747 - val_accuracy: 0.8333\n\nEpoch 00946: val_loss did not improve from 0.01806\nEpoch 947/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9499 - val_accuracy: 0.8333\n\nEpoch 00947: val_loss did not improve from 0.01806\nEpoch 948/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9092 - val_accuracy: 0.8333\n\nEpoch 00948: val_loss did not improve from 0.01806\nEpoch 949/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9273 - val_accuracy: 0.8333\n\nEpoch 00949: val_loss did not improve from 0.01806\nEpoch 950/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9492 - val_accuracy: 0.8333\n\nEpoch 00950: val_loss did not improve from 0.01806\nEpoch 951/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9361 - val_accuracy: 0.8333\n\nEpoch 00951: val_loss did not improve from 0.01806\nEpoch 952/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9476 - val_accuracy: 0.8333\n\nEpoch 00952: val_loss did not improve from 0.01806\nEpoch 953/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9441 - val_accuracy: 0.8333\n\nEpoch 00953: val_loss did not improve from 0.01806\nEpoch 954/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9662 - val_accuracy: 0.8333\n\nEpoch 00954: val_loss did not improve from 0.01806\nEpoch 955/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9672 - val_accuracy: 0.8333\n\nEpoch 00955: val_loss did not improve from 0.01806\nEpoch 956/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9962 - val_accuracy: 0.8333\n\nEpoch 00956: val_loss did not improve from 0.01806\nEpoch 957/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.0240 - val_accuracy: 0.8333\n\nEpoch 00957: val_loss did not improve from 0.01806\nEpoch 958/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9840 - val_accuracy: 0.8333\n\nEpoch 00958: val_loss did not improve from 0.01806\nEpoch 959/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9977 - val_accuracy: 0.8333\n\nEpoch 00959: val_loss did not improve from 0.01806\nEpoch 960/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.0315 - val_accuracy: 0.8333\n\nEpoch 00960: val_loss did not improve from 0.01806\nEpoch 961/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9960 - val_accuracy: 0.8333\n\nEpoch 00961: val_loss did not improve from 0.01806\nEpoch 962/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9824 - val_accuracy: 0.8333\n\nEpoch 00962: val_loss did not improve from 0.01806\nEpoch 963/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9944 - val_accuracy: 0.8333\n\nEpoch 00963: val_loss did not improve from 0.01806\nEpoch 964/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9823 - val_accuracy: 0.8333\n\nEpoch 00964: val_loss did not improve from 0.01806\nEpoch 965/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9911 - val_accuracy: 0.8333\n\nEpoch 00965: val_loss did not improve from 0.01806\nEpoch 966/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9674 - val_accuracy: 0.8333\n\nEpoch 00966: val_loss did not improve from 0.01806\nEpoch 967/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9501 - val_accuracy: 0.8333\n\nEpoch 00967: val_loss did not improve from 0.01806\nEpoch 968/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9453 - val_accuracy: 0.8333\n\nEpoch 00968: val_loss did not improve from 0.01806\nEpoch 969/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9175 - val_accuracy: 0.8333\n\nEpoch 00969: val_loss did not improve from 0.01806\nEpoch 970/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9256 - val_accuracy: 0.8333\n\nEpoch 00970: val_loss did not improve from 0.01806\nEpoch 971/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9244 - val_accuracy: 0.8333\n\nEpoch 00971: val_loss did not improve from 0.01806\nEpoch 972/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8812 - val_accuracy: 0.8333\n\nEpoch 00972: val_loss did not improve from 0.01806\nEpoch 973/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8670 - val_accuracy: 0.8333\n\nEpoch 00973: val_loss did not improve from 0.01806\nEpoch 974/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8823 - val_accuracy: 0.8333\n\nEpoch 00974: val_loss did not improve from 0.01806\nEpoch 975/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9099 - val_accuracy: 0.8333\n\nEpoch 00975: val_loss did not improve from 0.01806\nEpoch 976/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9500 - val_accuracy: 0.8333\n\nEpoch 00976: val_loss did not improve from 0.01806\nEpoch 977/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9638 - val_accuracy: 0.8333\n\nEpoch 00977: val_loss did not improve from 0.01806\nEpoch 978/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9913 - val_accuracy: 0.8333\n\nEpoch 00978: val_loss did not improve from 0.01806\nEpoch 979/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9783 - val_accuracy: 0.8333\n\nEpoch 00979: val_loss did not improve from 0.01806\nEpoch 980/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9757 - val_accuracy: 0.8333\n\nEpoch 00980: val_loss did not improve from 0.01806\nEpoch 981/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9644 - val_accuracy: 0.8333\n\nEpoch 00981: val_loss did not improve from 0.01806\nEpoch 982/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9572 - val_accuracy: 0.8333\n\nEpoch 00982: val_loss did not improve from 0.01806\nEpoch 983/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9613 - val_accuracy: 0.8333\n\nEpoch 00983: val_loss did not improve from 0.01806\nEpoch 984/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9695 - val_accuracy: 0.8333\n\nEpoch 00984: val_loss did not improve from 0.01806\nEpoch 985/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9208 - val_accuracy: 0.8333\n\nEpoch 00985: val_loss did not improve from 0.01806\nEpoch 986/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9291 - val_accuracy: 0.8333\n\nEpoch 00986: val_loss did not improve from 0.01806\nEpoch 987/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9378 - val_accuracy: 0.8333\n\nEpoch 00987: val_loss did not improve from 0.01806\nEpoch 988/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8858 - val_accuracy: 0.8333\n\nEpoch 00988: val_loss did not improve from 0.01806\nEpoch 989/1000\n23/23 [==============================] - 1s 26ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8630 - val_accuracy: 0.8333\n\nEpoch 00989: val_loss did not improve from 0.01806\nEpoch 990/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9025 - val_accuracy: 0.8333\n\nEpoch 00990: val_loss did not improve from 0.01806\nEpoch 991/1000\n23/23 [==============================] - 1s 26ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9086 - val_accuracy: 0.8333\n\nEpoch 00991: val_loss did not improve from 0.01806\nEpoch 992/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9198 - val_accuracy: 0.8333\n\nEpoch 00992: val_loss did not improve from 0.01806\nEpoch 993/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9085 - val_accuracy: 0.8333\n\nEpoch 00993: val_loss did not improve from 0.01806\nEpoch 994/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9292 - val_accuracy: 0.8333\n\nEpoch 00994: val_loss did not improve from 0.01806\nEpoch 995/1000\n23/23 [==============================] - 1s 26ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8859 - val_accuracy: 0.8333\n\nEpoch 00995: val_loss did not improve from 0.01806\nEpoch 996/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.8849 - val_accuracy: 0.8333\n\nEpoch 00996: val_loss did not improve from 0.01806\nEpoch 997/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9147 - val_accuracy: 0.8333\n\nEpoch 00997: val_loss did not improve from 0.01806\nEpoch 998/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9002 - val_accuracy: 0.8333\n\nEpoch 00998: val_loss did not improve from 0.01806\nEpoch 999/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9017 - val_accuracy: 0.8333\n\nEpoch 00999: val_loss did not improve from 0.01806\nEpoch 1000/1000\n23/23 [==============================] - 1s 27ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 8.9351 - val_accuracy: 0.8333\n\nEpoch 01000: val_loss did not improve from 0.01806\nTraining completed in time:  0:10:14.738812\n"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint \n",
    "from datetime import datetime \n",
    "\n",
    "num_epochs = 1000\n",
    "num_batch_size = 20\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='models/weights.best.basic_cnn1.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Training Accuracy:  1.0\nTesting Accuracy:  0.8333333134651184\n"
    }
   ],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy: \", score[1])\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Testing Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}